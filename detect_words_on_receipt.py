# -*- coding: utf-8 -*-
"""Detect Words on Receipt

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/detect-words-on-receipt-e07f3fd9-7a3d-400a-848a-8454534ee246.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240309/auto/storage/goog4_request%26X-Goog-Date%3D20240309T142721Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db2c71a7a227bd238156859d34caf9854ad03e9e9be06ff620aa83db7696ba9536763795b7542a7c471a368155b20f6c1f9b13e7b902ee4c2e80ffe41a7f405ffacebb3c543a12ae40b24aaf6d8e186049ca8dbee45ef5bcdbff27a8a45ee69159b9193028e0a0537bf1967d11da6d20e0c4691bc6ff9be817af6aa09e9663262274307a3a1e4ce6c9a21462422e7b3d9ca909f25d38d95e3a2c063a558d57c6dc57603296ba825f2260ddb6b1a2e53e7f1e3ef606308b656d92e036cbc93ef85baf1e762fac08d4bcf49d347bb0f8d4cfef7d86ba5ca4bc00304c974698c5b82b98d558eba1e782ba9e81b7bd00c491e4b448f8f9f3659c47ea0e72c3e70e3d2

Data Collection using Kaggle Dataset for Retail Receipts
"""

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'ocr-receipts-text-detection:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3753245%2F6494011%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240309%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240309T142721Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5aeed51c811437c000caea59ac428a5a1da90f9b65e91d6daeefd8ce429053b8c7d647812ec0896a1fb581a41e09583d6e00b27c66986cc8ebd4e6d68e8a4909e8fa8717742e7835a73a789558001a6f4c4c93283b9415002aff37cb285a4380cc462fc3da2309c3300c033312ca6d960002c8888810cc0b137d5e3867314a3879e5bc86ec69ca0ccd180061b5c1e71df5af1c0af30e26691ee0eb14366199e870a4e3fe0721a0c76e701f54e0a7558821f3a83745506f0eba2e2e81307a653dc537d1e10301f1a862edf8733887af629e4d543105152a8c5254f00f7314ea7e7a62d9cad6b1ca5303a0ce97fada44948bbb7d7dbfa846dd567d13e42d328f2f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

pip install easyocr

import numpy as np
import json
import matplotlib.pyplot as plt
import cv2
import os
from bs4 import BeautifulSoup
import torch
from keras import layers
from keras.models import Model
from sklearn.model_selection import train_test_split

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

W,H = 320, 320

"""**Data Pre-processing using Gaussian Threshold**

>For data Pre-processing, I've used Gaussian Threshold method and customised my dataset and stored the processed images locally and then uploaded them to the drive
"""

import os
import cv2

def gaussian_threshold(image_path, output_folder, threshold_value=55, max_value=255):
    # Load the image
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Apply Gaussian thresholding
    _, thresholded_image = cv2.threshold(image, threshold_value, max_value, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Save the thresholded image
    filename = os.path.basename(image_path)
    output_path = os.path.join(output_folder, filename)
    cv2.imwrite(output_path, thresholded_image)

def gaussian_threshold_all_images(input_folder, output_folder, threshold_value=55, max_value=255):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Process each image in the input folder
    for filename in os.listdir(input_folder):
        if filename.endswith(".jpg") or filename.endswith(".png"):  # Adjust as per your image types
            image_path = os.path.join(input_folder, filename)
            gaussian_threshold(image_path, output_folder, threshold_value, max_value)

# Example usage:
input_folder = 'BIllReceiptImageCollection'
output_folder = 'outputfolder'
gaussian_threshold_all_images(input_folder, output_folder, threshold_value=127, max_value=255)

from google.colab import drive
drive.mount('/content/drive')

"""Use pre-processed images for model training"""

data_path = '/content/drive/MyDrive/imgsocr/imgs'
box_path = '/kaggle/input/ocr-receipts-text-detection/boxes'
xml_path = '/kaggle/input/ocr-receipts-text-detection/annotations.xml'
east_path = '/kaggle/input/east-model/frozen_east_text_detection.pb'

determined_labels = {'shop': 0, 'item': 1, 'total': 2, 'date_time': 3, 'other': 4}

class ExtractingText:

    def __init__(self, solution: str, usingGPU: bool):

        self.gpu = usingGPU
        if 'Terr' or 'Easy' in solution:
            self.sol = solution
        else:
            print(f"Solution must be 'Tess' or Easy")


    def cleanup_text(self, text):
        # strip out non-ASCII text so we can draw the text on the image
        # using OpenCV
        return "".join([c if ord(c) < 128 else "" for c in text]).strip()

    """
    Tesseract
    """
    def Tesseract(self, dtset, min_length):
        if 'Tess' in self.sol:
            import pytesseract
            from PIL import Image

            char = []
            for idx in dtset:
                z = []
                image = Image.open(os.path.join(data_path, idx))
                data = pytesseract.image_to_data(image,
                                                 output_type=pytesseract.Output.DICT)

                for i in range(len(data['text'])):
                    text = data['text'][i]
                    text = self.cleanup_text(text)
                    if len(text) < min_length: continue
                    z.append([text])

                char.append([idx, z])
            return char

        else:
            print("Solution must be 'Tess'")

    def get_results(self, ocr_results, de_prob):
        words = []
        for (bbox, text, prob) in ocr_results:
            if prob > de_prob and len(text)>2:
                text = self.cleanup_text(text)  # Assuming cleanup_text is defined elsewhere
                words.append([text])
        return words

    def EasyOCR(self, dtset, languages, de_prob=0.5):
        if 'Easy' in self.sol:
            from easyocr import Reader
            char_pos = []
            for idx in dtset:
                img_file = os.path.join(data_path, os.path.basename(idx))
                img = cv2.imread(img_file)
                reader = Reader([languages], gpu=self.gpu)
                results = reader.readtext(img)
                bboxes = self.get_results(results, de_prob)
                char_pos.append([idx, bboxes])
            return char_pos
        else:
            print("Solution must be 'Easy'")

extract_text = ExtractingText('Easy', True)
word_position = extract_text.EasyOCR(sorted(os.listdir(data_path)), 'en', de_prob=0.2)

def get_data(annotation_file, ext_word, split_rate=0.2):

    dt_set = []

    with open(annotation_file, 'r') as f:
            data = f.read()
    Bs_data = BeautifulSoup(data, "xml")
    infors = Bs_data.find_all('image')

    for i, idx in enumerate(infors):
        k = []
        z = []
        org_dt = []
        name_file = idx.get('name')
        result = [inner_list[0] for inner_list in ext_word[i][1]]
        boxes = idx.find_all('box')

        for box in boxes:
            attribute_element = box.find('attribute', {'name': 'text'})
            text_content = attribute_element.text if attribute_element else None
            label = box.get('label')
            k.append((text_content, label))
            z.append([text_content, label])
            org_dt = z.copy()

        for x in result:
            match_found = False
            for i, j in k:
                if x in i:
                    z.append([x, j])
                    match_found = True
                    break  # Exit the inner loop once a match is found

            if not match_found:
                z.append([x, 'other'])


        dt_set.append([name_file, z + org_dt])

    train_dt, val_dt = train_test_split(dt_set, test_size=split_rate)

    return train_dt, val_dt

train_ds, val_ds = get_data(xml_path, word_position, split_rate=0.4)

"""# Using DistilBert to classify Text using NLP"""

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

import torch.nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

BATCH_SIZE = 32

def get_NLP_data(ds):

    X=[]; y = []
    for index in train_ds:
        for inf in index[1]:
            text = inf[0]
            label = determined_labels[inf[1]]
            X.append(text)
            y.append(label)

    return X, y

X_train, y_train = get_NLP_data(train_ds)
X_val, y_val = get_NLP_data(val_ds)

model_name = 'distilbert-base-uncased'
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

train_data = tokenizer(list(X_train), return_tensors="pt", padding=True, truncation=True)
valid_data = tokenizer(list(X_val), return_tensors="pt", padding=True, truncation=True)

class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': torch.tensor(self.labels[idx])
        }

# Create dataset and dataloader
train_dataset = SentimentDataset(
    input_ids=train_data['input_ids'],
    attention_mask=train_data['attention_mask'],
    labels=y_train
)

val_dataset = SentimentDataset(
    input_ids=valid_data['input_ids'],
    attention_mask=valid_data['attention_mask'],
    labels=y_val
)

class SaveBestModelCallback:
    def __init__(self, model, save_path):
        self.model = model
        self.save_path = save_path
        self.best_val_loss = float(0.65)

    def __call__(self, val_loss):
        if val_loss > self.best_val_loss:
            print(f"Validation loss improved ({self.best_val_loss:.6f} --> {val_loss:.6f}). Saving model...")
            torch.save(self.model.state_dict(), self.save_path)
            self.best_val_loss = val_loss
        else:
            print(f"Validation loss did not improve ({self.best_val_loss:.6f} --> {val_loss:.6f}).")

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)

model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels = 5)
model = model.to(device)

save_callback = SaveBestModelCallback(model, '/kaggle/working/best_model.pth')
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = torch.nn.CrossEntropyLoss()

from tqdm import tqdm

num_epochs = 30

for epoch in range(num_epochs):
    # Training loop
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    progress_bar = tqdm(enumerate(train_dataloader, 1), total=len(train_dataloader))
    for step, batch in progress_bar:
        batch = {key: value.to(device) for key, value in batch.items()}
        optimizer.zero_grad()
        outputs = model(**batch)
        loss = criterion(outputs.logits, batch['labels'])
        loss.backward()
        optimizer.step()

        # Calculate accuracy
        predictions = torch.argmax(outputs.logits, dim=1)
        correct = (predictions == batch['labels']).sum().item()
        total_correct += correct
        total_samples += len(batch['labels'])

        # Accumulate loss
        total_loss += loss.item()

        progress_bar.set_description(f'Epoch {epoch+1}/{num_epochs}, Step {step}/{len(train_dataloader)}')
        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct / len(batch['labels'])})

    # Calculate training statistics for the epoch
    epoch_loss = total_loss / len(train_dataloader)
    epoch_accuracy = total_correct / total_samples

    # Validation loop
    model.eval()
    val_total_loss = 0
    val_total_correct = 0
    val_total_samples = 0

    with torch.no_grad():
        for val_step, val_batch in enumerate(val_dataloader, 1):
            val_batch = {key: value.to(device) for key, value in val_batch.items()}
            val_outputs = model(**val_batch)
            val_loss = criterion(val_outputs.logits, val_batch['labels'])

            # Calculate accuracy
            val_predictions = torch.argmax(val_outputs.logits, dim=1)
            val_correct = (val_predictions == val_batch['labels']).sum().item()
            val_total_correct += val_correct
            val_total_samples += len(val_batch['labels'])

            # Accumulate loss
            val_total_loss += val_loss.item()

    # Calculate validation statistics for the epoch
    val_epoch_loss = val_total_loss / len(val_dataloader)
    val_epoch_accuracy = val_total_correct / val_total_samples

    save_callback(val_epoch_accuracy)

    # Print statistics for the epoch
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_accuracy:.4f}')

pred_values = []
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels = 5)
try:
    model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))
except:
    model = model

for i in X_val:
    test_data = tokenizer(i, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**test_data)
        logits = outputs.logits

    # Convert logits to probabilities and get predicted label
    probabilities = torch.softmax(logits, dim=-1)
    predicted_label = torch.argmax(probabilities, dim=-1).item()

    pred_values.append(predicted_label)

conf_matrix = confusion_matrix(y_val, pred_values)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels = 5)
model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))

def easyOCR_results(image, title, min_len):
    from easyocr import Reader
    img = cv2.imread(image)
    reader = Reader(['en'], gpu=True)
    results = reader.readtext(img)

    for (bbox, text, prob) in results:
        if prob > int(min_len/100) and len(text) > min_len:
            (tl, tr, br, bl) = bbox
            tl = (int(tl[0]), int(tl[1]))
            tr = (int(tr[0]), int(tr[1]))
            br = (int(br[0]), int(br[1]))
            bl = (int(bl[0]), int(bl[1]))

            text = extract_text.cleanup_text(text)
            tknz = tokenizer(text, return_tensors="pt")

            with torch.no_grad():
                outputs = model(**tknz)
                logits = outputs.logits

            probabilities = torch.softmax(logits, dim=-1)
            predicted_label = torch.argmax(probabilities, dim=-1).item()

            for label, value in determined_labels.items():
                 if value == predicted_label:
                        text = label

            if 'shop' in text:
                cv2.rectangle(img, tl, br, (255, 0, 0), 2)
                cv2.putText(img, text, (tl[0], tl[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

            elif 'item' in text:
                cv2.rectangle(img, tl, br, (0, 175, 0), 2)
                cv2.putText(img, text, (tl[0], tl[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 175, 0), 2)
            elif 'total' in text:
                cv2.rectangle(img, tl, br, (0, 0, 255), 2)
                cv2.putText(img, text, (tl[0], tl[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

            elif 'other' in text:
                cv2.rectangle(img, tl, br, (0, 0, 0), 2)
                cv2.putText(img, text, (tl[0], tl[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)

            else:
                cv2.rectangle(img, tl, br, (255, 175, 0), 2)
                cv2.putText(img, text, (tl[0], tl[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 175, 0), 2)

    plt.title(title)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.show()

def Tess_results(image, title, min_len):
    import pytesseract
    img = cv2.imread(image)
    data = pytesseract.image_to_data(image,
                                     output_type=pytesseract.Output.DICT)

    for i in range(len(data['text'])):
        text = data['text'][i]
        text = extract_text.cleanup_text(text)
        if len(text) < min_len: continue

        tknz = tokenizer(text, return_tensors="pt")

        with torch.no_grad():
            outputs = model(**tknz)
            logits = outputs.logits

        probabilities = torch.softmax(logits, dim=-1)
        predicted_label = torch.argmax(probabilities, dim=-1).item()

        for label, value in determined_labels.items():
             if value == predicted_label:
                    text = label

        x, y, width, height = int(data['left'][i]), int(data['top'][i]), int(data['width'][i]), int(data['height'][i])

        if 'shop' in text:
            cv2.rectangle(img, (x, y), (x + width, y + height), (255, 0, 0), 2)
            cv2.putText(img, text, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)


        elif 'item' in text:
                cv2.rectangle(img, (x, y), (x + width, y + height), (0, 175, 0), 2)
                cv2.putText(img, text, (x, y - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 175, 0), 2)
        elif 'total' in text:
            cv2.rectangle(img, (x, y), (x + width, y + height), (0, 0, 255), 2)
            cv2.putText(img, text, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        elif 'other' in text:
            cv2.rectangle(img, (x, y), (x + width, y + height), (0, 0, 0), 2)
            cv2.putText(img, text, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)

        else:
            cv2.rectangle(img, (x, y), (x + width, y + height), (255, 175, 0), 2)
            cv2.putText(img, text, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 175, 0), 2)

    plt.title(title)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.show()

def display_results(test_dt, method='Easy', min_len=5):
    file_val = [inner_list[0] for inner_list in test_dt]
    if method == 'Easy':
        for idx in file_val:
            img_file = os.path.join(data_path, os.path.basename(idx))
            easyOCR_results(img_file, idx, min_len)

    elif method == 'Tess':
        for idx in file_val:
            img_file = os.path.join(data_path, os.path.basename(idx))
            Tess_results(img_file, idx, min_len)

    else:
        print("Method must be 'Tess' or 'Easy'")

display_results(val_ds, method='Easy', min_len=5)

"""# Donut Model"""

pip install -q transformers datasets sentencepiece

pip install -q pytorch-lightning wandb

pip install datasets --upgrade

from datasets import load_dataset

dataset = load_dataset("naver-clova-ix/cord-v2")

"""Using DistilBert detecting the text and objectifying to JSON object
>Displaying image for reference
"""

import re

from transformers import DonutProcessor, VisionEncoderDecoderModel
from datasets import load_dataset
import torch
import cv2

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
# load document image
image = cv2.imread('/content/drive/MyDrive/imgsocr/imgs/receipt_image_110.jpg')
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
# prepare decoder inputs
task_prompt = "<s_cord-v2>"
decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids

pixel_values = processor(image, return_tensors="pt").pixel_values

outputs = model.generate(
    pixel_values.to(device),
    decoder_input_ids=decoder_input_ids.to(device),
    max_length=model.decoder.config.max_position_embeddings,
    pad_token_id=processor.tokenizer.pad_token_id,
    eos_token_id=processor.tokenizer.eos_token_id,
    use_cache=True,
    bad_words_ids=[[processor.tokenizer.unk_token_id]],
    return_dict_in_generate=True,
)

sequence = processor.batch_decode(outputs.sequences)[0]
sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()  # remove first task start token
print(processor.token2json(sequence))

"""Code to save the detections in JSON File"""

import re
import json
from transformers import DonutProcessor, VisionEncoderDecoderModel
from datasets import load_dataset
import torch
import cv2

processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Load document image
image_path = '/content/drive/MyDrive/imgsocr/imgs/receipt_image_110.jpg'
image = cv2.imread(image_path)

# Prepare decoder inputs
task_prompt = "<s_cord-v2>"
decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids

# Process the image
pixel_values = processor(image, return_tensors="pt").pixel_values

# Generate sequence
outputs = model.generate(
    pixel_values.to(device),
    decoder_input_ids=decoder_input_ids.to(device),
    max_length=model.decoder.config.max_position_embeddings,
    pad_token_id=processor.tokenizer.pad_token_id,
    eos_token_id=processor.tokenizer.eos_token_id,
    use_cache=True,
    bad_words_ids=[[processor.tokenizer.unk_token_id]],
    return_dict_in_generate=True,
)

# Decode the sequence
sequence = processor.batch_decode(outputs.sequences)[0]
sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()  # remove first task start token

print("boxes:",sequence)


# # Save bounding box coordinates and sequence in JSON
output_data = {"sequence": sequence}
output_json_path = '/content/drive/MyDrive/imgsocr/output/output.json'
with open(output_json_path, 'w') as json_file:
    json.dump(output_data, json_file)

print("Output image with bounding boxes and JSON file saved successfully.")

"""
**API Code for inference**

"""

import os
import re
import json
from flask import Flask, request, jsonify
from transformers import DonutProcessor, VisionEncoderDecoderModel
import torch
import cv2

app = Flask(__name__)

# Load model and processor
processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

@app.route('/api/extract_sequence', methods=['POST'])
def extract_sequence():
    # Check if image file is present in the request
    if 'image' not in request.files:
        return jsonify({'error': 'No image provided'}), 400

    # Get the image file from the request
    image_file = request.files['image']

    # Save the image temporarily
    temp_image_path = 'temp_image.jpg'
    image_file.save(temp_image_path)

    # Load the image
    image = cv2.imread(temp_image_path)

    # Prepare decoder inputs
    task_prompt = "<s_cord-v2>"
    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids

    # Process the image
    pixel_values = processor(image, return_tensors="pt").pixel_values

    # Generate sequence
    with torch.no_grad():
        outputs = model.generate(
            pixel_values.to(device),
            decoder_input_ids=decoder_input_ids.to(device),
            max_length=model.decoder.config.max_position_embeddings,
            pad_token_id=processor.tokenizer.pad_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
            use_cache=True,
            bad_words_ids=[[processor.tokenizer.unk_token_id]],
            return_dict_in_generate=True,
        )

    # Decode the sequence
    sequence = processor.batch_decode(outputs.sequences)[0]
    sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
    sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()  # remove first task start token

    # Remove the temporary image file
    os.remove(temp_image_path)

    # Create JSON response
    response = {'sequence': sequence}
    return jsonify(response)

if __name__ == '__main__':
    app.run(debug=True)

